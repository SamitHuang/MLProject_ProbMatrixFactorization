{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import auc, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.estimator_checks import check_estimator\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "K=2\n",
    "lamU=0.1\n",
    "lamV=0.1\n",
    "\n",
    "lr = 0.001 #step size i.e. learning rate\n",
    "MAX_ITER = 10 #200\n",
    "\n",
    "path = \"../dataset/data.npz\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['item_id', 'user_id', 'rating']"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npzfile.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data combination and split. implement based on numpy API\n",
    "#\n",
    "dataAll = np.asarray([npzfile['user_id'].reshape(-1), npzfile['item_id'].reshape(-1), npzfile['rating'].reshape(-1)] )\n",
    "dataAll=dataAll.T\n",
    "[USER_ID_MAX, ITEM_ID_MAX, _] = np.max(dataAll, axis=0) #TODO: n unique\n",
    "trainData, testData = train_test_split(dataAll, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "R_train = dataToMatrix(trainData)\n",
    "indicator = (R_train > 0).astype(int)\n",
    "np.random.seed(seed=1) # for reproducible\n",
    "#initialize params\n",
    "U= np.random.random_sample((K, USER_ID_MAX))\n",
    "V= np.random.random_sample((K, ITEM_ID_MAX))\n",
    "\n",
    "R_test = dataToMatrix(testData)\n",
    "indicatoR_test = (R_test>0).astype(int)\n",
    "\n",
    "\n",
    "R_pred = U.T @ V # matmul, TODO: logistic function to overcome rating out range\n",
    "diff = indicator * (R_train - R_pred)\n",
    "\n",
    "rmseTrain=[]\n",
    "rmseTest=[]\n",
    "\n",
    "\n",
    "for epoch in range(MAX_ITER):\n",
    "     #TODO: Linear with # of rating. take advantage of the sparsity of the matrix.\n",
    "    dEdU = -V @ diff.T  + lamU * U\n",
    "    dEdV = -U @ diff  + lamV * V\n",
    "    # sync update \n",
    "    Ut = U - lr * dEdU #TODO:  - or +\n",
    "    Vt = V - lr * dEdV\n",
    "    U = Ut\n",
    "    V = Vt\n",
    "    \n",
    "    R_pred = U.T @ V #matmul, TODO: logistic function to overcome rating out range\n",
    "    diff = indicator * (R_train - R_pred)\n",
    "    \n",
    "    # evaluate RMSE loss\n",
    "    rmseTrain.append(np.sqrt( (diff**2).sum()/indicator.sum()))\n",
    "    diffTest = indicatoR_test * (R_test - R_pred)\n",
    "    rmseTest.append(np.sqrt( (diffTest**2).sum()/indicatoR_test.sum()))\n",
    "    print(\"Epoch {}: Train RMSE: {:.4f}\\t Test RMSE: {:.4f}\".format(epoch, rmseTrain[epoch], rmseTest[epoch]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train RMSE: 3.1344\n",
      "Epoch 1: Train RMSE: 3.0064\n",
      "Epoch 2: Train RMSE: 2.8634\n",
      "Epoch 3: Train RMSE: 2.7085\n",
      "Epoch 4: Train RMSE: 2.5466\n",
      "Epoch 5: Train RMSE: 2.3841\n",
      "Epoch 6: Train RMSE: 2.2280\n",
      "Epoch 7: Train RMSE: 2.0842\n",
      "Epoch 8: Train RMSE: 1.9563\n",
      "Epoch 9: Train RMSE: 1.8457\n",
      "Epoch 10: Train RMSE: 1.7515\n",
      "Epoch 11: Train RMSE: 1.6718\n",
      "Epoch 12: Train RMSE: 1.6039\n",
      "Epoch 13: Train RMSE: 1.5456\n",
      "Epoch 14: Train RMSE: 1.4950\n",
      "Epoch 15: Train RMSE: 1.4507\n",
      "Epoch 16: Train RMSE: 1.4114\n",
      "Epoch 17: Train RMSE: 1.3763\n",
      "Epoch 18: Train RMSE: 1.3447\n",
      "Epoch 19: Train RMSE: 1.3162\n",
      "Epoch 20: Train RMSE: 1.2903\n",
      "Epoch 21: Train RMSE: 1.2667\n",
      "Epoch 22: Train RMSE: 1.2451\n",
      "Epoch 23: Train RMSE: 1.2252\n",
      "Epoch 24: Train RMSE: 1.2069\n",
      "Epoch 25: Train RMSE: 1.1901\n",
      "Epoch 26: Train RMSE: 1.1744\n",
      "Epoch 27: Train RMSE: 1.1599\n",
      "Epoch 28: Train RMSE: 1.1464\n",
      "Epoch 29: Train RMSE: 1.1339\n",
      "Epoch 30: Train RMSE: 1.1221\n",
      "Epoch 31: Train RMSE: 1.1112\n",
      "Epoch 32: Train RMSE: 1.1009\n",
      "Epoch 33: Train RMSE: 1.0913\n",
      "Epoch 34: Train RMSE: 1.0822\n",
      "Epoch 35: Train RMSE: 1.0737\n",
      "Epoch 36: Train RMSE: 1.0656\n",
      "Epoch 37: Train RMSE: 1.0581\n",
      "Epoch 38: Train RMSE: 1.0509\n",
      "Epoch 39: Train RMSE: 1.0441\n",
      "Epoch 40: Train RMSE: 1.0377\n",
      "Epoch 41: Train RMSE: 1.0316\n",
      "Epoch 42: Train RMSE: 1.0259\n",
      "Epoch 43: Train RMSE: 1.0204\n",
      "Epoch 44: Train RMSE: 1.0152\n",
      "Epoch 45: Train RMSE: 1.0102\n",
      "Epoch 46: Train RMSE: 1.0055\n",
      "Epoch 47: Train RMSE: 1.0010\n",
      "Epoch 48: Train RMSE: 0.9967\n",
      "Epoch 49: Train RMSE: 0.9926\n",
      "Epoch 50: Train RMSE: 0.9887\n",
      "Epoch 51: Train RMSE: 0.9849\n",
      "Epoch 52: Train RMSE: 0.9813\n",
      "Epoch 53: Train RMSE: 0.9779\n",
      "Epoch 54: Train RMSE: 0.9746\n",
      "Epoch 55: Train RMSE: 0.9714\n",
      "Epoch 56: Train RMSE: 0.9684\n",
      "Epoch 57: Train RMSE: 0.9655\n",
      "Epoch 58: Train RMSE: 0.9627\n",
      "Epoch 59: Train RMSE: 0.9600\n",
      "Epoch 60: Train RMSE: 0.9574\n",
      "Epoch 61: Train RMSE: 0.9548\n",
      "Epoch 62: Train RMSE: 0.9524\n",
      "Epoch 63: Train RMSE: 0.9501\n",
      "Epoch 64: Train RMSE: 0.9479\n",
      "Epoch 65: Train RMSE: 0.9457\n",
      "Epoch 66: Train RMSE: 0.9436\n",
      "Epoch 67: Train RMSE: 0.9416\n",
      "Epoch 68: Train RMSE: 0.9396\n",
      "Epoch 69: Train RMSE: 0.9377\n",
      "Epoch 70: Train RMSE: 0.9359\n",
      "Epoch 71: Train RMSE: 0.9341\n",
      "Epoch 72: Train RMSE: 0.9324\n",
      "Epoch 73: Train RMSE: 0.9307\n",
      "Epoch 74: Train RMSE: 0.9291\n",
      "Epoch 75: Train RMSE: 0.9276\n",
      "Epoch 76: Train RMSE: 0.9260\n",
      "Epoch 77: Train RMSE: 0.9246\n",
      "Epoch 78: Train RMSE: 0.9231\n",
      "Epoch 79: Train RMSE: 0.9218\n",
      "Epoch 80: Train RMSE: 0.9204\n",
      "Epoch 81: Train RMSE: 0.9191\n",
      "Epoch 82: Train RMSE: 0.9178\n",
      "Epoch 83: Train RMSE: 0.9166\n",
      "Epoch 84: Train RMSE: 0.9154\n",
      "Epoch 85: Train RMSE: 0.9142\n",
      "Epoch 86: Train RMSE: 0.9130\n",
      "Epoch 87: Train RMSE: 0.9119\n",
      "Epoch 88: Train RMSE: 0.9108\n",
      "Epoch 89: Train RMSE: 0.9098\n",
      "Epoch 90: Train RMSE: 0.9087\n",
      "Epoch 91: Train RMSE: 0.9077\n",
      "Epoch 92: Train RMSE: 0.9067\n",
      "Epoch 93: Train RMSE: 0.9058\n",
      "Epoch 94: Train RMSE: 0.9048\n",
      "Epoch 95: Train RMSE: 0.9039\n",
      "Epoch 96: Train RMSE: 0.9030\n",
      "Epoch 97: Train RMSE: 0.9021\n",
      "Epoch 98: Train RMSE: 0.9013\n",
      "Epoch 99: Train RMSE: 0.9004\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0470970883356563"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training and infer without cross validation\n",
    "# note that idx starts from 0. user_id=1 ->  row 0 in the matrix\n",
    "def dataToMatrix(data, dim=[USER_ID_MAX, ITEM_ID_MAX]):\n",
    "    matrix = np.zeros(dim)\n",
    "    for ins in data:\n",
    "        matrix[ins[0]-1,ins[1]-1] = ins[2]\n",
    "    return matrix\n",
    "\n",
    "#rawData -input data with format [[userId, itemId, rating]xN]\n",
    "def PMF_test(rawData, U, V, useSparse=True):\n",
    "    #TODO: use sparsity.\n",
    "    if(not useSparse):\n",
    "        R_test = dataToMatrix(rawData)\n",
    "        indicatoR_test = (R_test>0).astype(int)\n",
    "        R_pred = U.T @ V \n",
    "        diff = indicatoR_test * (R_test - R_pred)\n",
    "        rmse = np.sqrt( (diff**2).sum()/indicatoR_test.sum())\n",
    "    else:\n",
    "        mseSum = 0\n",
    "        for (uID, iID, rij) in rawData:\n",
    "            i = uID - 1\n",
    "            j = iID - 1\n",
    "            mseSum += (rij - U.T[i,:]@V[:,j])**2\n",
    "        rmse = np.sqrt(mseSum/len(rawData))\n",
    "    return rmse\n",
    "\n",
    "\n",
    "#rawData -input data with format [[userId, itemId, rating]xN]\n",
    "#K - the number of latent features\n",
    "def PMF_train(rawData, maxIter=200, K=2,  lamU=0.1, lamV=0.1, useSparse=False ,verbose=0):\n",
    "    \n",
    "    #initialize params. \n",
    "    np.random.seed(seed=1)\n",
    "    U= np.random.random_sample((K, USER_ID_MAX)) \n",
    "    V= np.random.random_sample((K, ITEM_ID_MAX))\n",
    "    n_iter = 0\n",
    "    \n",
    "    if(not useSparse):\n",
    "        # Matrix Operations\n",
    "        R_train = dataToMatrix(rawData)\n",
    "        indicator = (R_train > 0).astype(int)\n",
    "\n",
    "        R_pred = U.T @ V # matmul, TODO: logistic function to overcome rating out range\n",
    "        diff = indicator * (R_train - R_pred)\n",
    "\n",
    "        rmseTrain=[]\n",
    "        for epoch in range(maxIter):\n",
    "             #TODO: Linear with # of rating. take advantage of the sparsity of the matrix.\n",
    "            dEdU = -V @ diff.T  + lamU * U\n",
    "            dEdV = -U @ diff  + lamV * V\n",
    "            # sync update \n",
    "            Ut = U - lr * dEdU #TODO:  - or +\n",
    "            Vt = V - lr * dEdV\n",
    "            U = Ut\n",
    "            V = Vt\n",
    "\n",
    "            R_pred = U.T @ V #matmul, TODO: logistic function to overcome rating out range\n",
    "            diff = indicator * (R_train - R_pred)\n",
    "\n",
    "            n_iter +=1\n",
    "\n",
    "            rmseTrain.append(np.sqrt( (diff**2).sum()/indicator.sum()))\n",
    "            # evaluate RMSE loss\n",
    "            if(verbose):\n",
    "                print(\"Epoch {}: Train RMSE: {:.4f}\".format(epoch, rmseTrain[epoch]))\n",
    "    else:\n",
    "        #Running Time is Linear to observed rating\n",
    "        rmseTrain = np.zeros(maxIter)\n",
    "        for epoch in range(maxIter):\n",
    "            Ut = U\n",
    "            Vt = V\n",
    "            for (uID,iID,rij) in trainData:\n",
    "                i = uID - 1\n",
    "                j = iID - 1\n",
    "                #partially update the latent features of user i and item j\n",
    "                diff_ij = rij - (U.T)[i,:] @ V[:,j]\n",
    "                Ut[:,i] += - lr * (-V[:,j] * diff_ij) \n",
    "                Vt[:,j] += - lr * (-U[:,i] * diff_ij)\n",
    "                rmseTrain[epoch] += diff_ij**2\n",
    "            #update U and V\n",
    "            Ut = Ut - lr * ( lamU * U)\n",
    "            Vt = Vt - lr * (lamV * V)\n",
    "            U = Ut\n",
    "            V = Vt\n",
    "            \n",
    "            rmseTrain[epoch] = np.sqrt(rmseTrain[epoch]/len(trainData))\n",
    "            n_iter+=1\n",
    "            if(verbose):\n",
    "                print(\"Epoch {}: Train RMSE: {:.4f}\".format(epoch, rmseTrain[epoch]))\n",
    "        \n",
    "    return U,V, n_iter, rmseTrain[n_iter-1]\n",
    "    \n",
    "U,V,n_iter, rmse = PMF_train(testData,K=2, maxIter=100, useSparse=False, verbose=1)\n",
    "PMF_test(trainData, U, V)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class PMF(BaseEstimator,TransformerMixin): #TODO: Transformer needed?\n",
    "    def __init__(self, maxIter=200, K=2, lamU=0.1, lamV=0.1 ):\n",
    "        self.maxIter = maxIter\n",
    "        self.K = K\n",
    "        self.lamU = lamU\n",
    "        self.lamV = lamV\n",
    "        \n",
    "        self.U = np.random.random_sample((K, USER_ID_MAX))\n",
    "        self.V = np.random.random_sample((K, ITEM_ID_MAX))\n",
    "        \n",
    "    \n",
    "    # interface for estimator\n",
    "    def fit(self, X, y=None, **params):\n",
    "        U, V, n_iter_, train_rmse_ = PMF_train(X, maxIter=self.maxIter, K=self.K, lamU=self.lamU, lamV=self.lamV)\n",
    "        \n",
    "        #parameters with trailing _ is used to check if the estimator has been fitted\n",
    "        #TODO: add validation rmse\n",
    "        self.rmse_=  train_rmse_\n",
    "        self.n_iter_ = n_iter_\n",
    "        self.U = U\n",
    "        self.V = V\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    # interface for Grid Search\n",
    "    def score(self, X, y=None):\n",
    "        rmse = PMF_test(X, self.U, self.V)\n",
    "        #print(\"Scoring: K={}, lamda U={}, lamda V={}, rmse={}\".format(self.K,self.lamU, self.lamV, rmse) )\n",
    "        #since build-in gridsearch pick params by \"the bigger the better\"\n",
    "        return -rmse\n",
    "        \n",
    "def GridSearchTunning(trainData, maxIter=100, verbose=0):\n",
    "    #Tune regularization hyper-params\n",
    "    regu_params = {\"lamU\":[0.1,1,10,100],\"lamV\":[0.1,1,10,100]}\n",
    "    pmfEst = PMF(K=2, maxIter=maxIter)\n",
    "    #splitting data into train/validation set by 5-fold \n",
    "    gs=GridSearchCV(pmfEst, regu_params, cv=5, refit=True, verbose=verbose)\n",
    "    gs.fit(trainData)\n",
    "\n",
    "    bestLamU = gs.best_params_['lamU']\n",
    "    bestLamV = gs.best_params_['lamV']\n",
    "    #Mean cross-validated score of the best_estimator\n",
    "    bestScoreL = -gs.best_score_ \n",
    "    print(\"Finish tunning lamda U and lamda V \\r\\n==> Best lamU {}, best lamV {}, RMSE={}\".format(bestLamU, bestLamV,bestScoreL ))\n",
    "    \n",
    "    #Tune # latent features\n",
    "    factors_params = {\"K\":[1,2,3,4,5]}\n",
    "    pmfEst2 = PMF(lamU=bestLamU,lamV=bestLamV, maxIter=maxIter)\n",
    "    gs2 =GridSearchCV(pmfEst, factors_params, cv=5, refit=True, verbose=verbose)\n",
    "    gs2.fit(trainData)\n",
    "\n",
    "    bestK = gs2.best_params_['K']\n",
    "    bestScoreK = -gs2.best_score_\n",
    "    print(\"Finish tunning factors K \\r\\n==> Best K={}, RMSE={}\".format(bestK,bestScoreK ))\n",
    "    \n",
    "    if(verbose==2):\n",
    "        print(\"CV details:{}\\r\\n{}\".format(gs.cv_results_, gs2.cv_results_) )\n",
    "    \n",
    "    return bestLamU, bestLamV, bestK\n",
    "\n",
    "#experiment on a given train data and test data\n",
    "def Experiment(trainData, testData, verbose=0):\n",
    "    #Grid Search Tunning on training set\n",
    "    bestLamU, bestLamV, bestK = GridSearchTunning(trainData, maxIter=100, verbose=verbose)\n",
    "    #train with the full training set using the tuned best hyper-params\n",
    "    U,V,_,rmse_train = PMF_train(trainData, K=bestK, lamU=bestLamU, lamV=bestLamV, verbose=1 )\n",
    "    #evaluate on test set\n",
    "    rmse_test = PMF_test(testData, U,V)\n",
    "    \n",
    "    return rmse_test\n",
    "    \n",
    "def main():\n",
    "    \n",
    "    trainData, testData = train_test_split(dataAll, test_size=0.2, random_state=0)\n",
    "    # Dense training data\n",
    "    rmse1 = Experiment(trainData, testData)\n",
    "    print(\"RMSE of test set on dense data: {:.4f}\".format(rmse1))\n",
    "    # Sparse training data\n",
    "    rmse2 = Experiment(testData, trainData)\n",
    "    print(\"RMSE of test set on sparse data: {:.4f}\".format(rmse2))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  80 out of  80 | elapsed:  3.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best lamU 0.1, best lamV 0.1, RMSE=-0.9571893631715415\n",
      "{'lamU': 0.1, 'lamV': 0.1}\n",
      "{'split0_test_score': array([-0.95849969, -0.95908187, -0.97517916, -1.91647734, -0.95944245,\n",
      "       -0.96026188, -0.97825638, -1.24697082, -0.97571203, -0.97841153,\n",
      "       -1.01374915, -1.37724476, -1.38552123, -2.02520265, -1.31341168,\n",
      "       -1.83076559]), 'split1_test_score': array([-0.95061414, -0.95149337, -0.97050946, -3.01952477, -0.95152726,\n",
      "       -0.95265752, -0.97364229, -1.33661307, -0.96825875, -0.97134193,\n",
      "       -1.0099879 , -1.88824764, -2.19279159, -1.39581863, -1.36210226,\n",
      "       -1.83782816]), 'split2_test_score': array([-0.95151775, -0.9519353 , -0.96721808, -1.53972565, -0.95224076,\n",
      "       -0.95290902, -0.97017395, -1.26512344, -0.96746673, -0.97009028,\n",
      "       -1.00517737, -1.43262981, -3.33790947, -1.2773578 , -1.27041385,\n",
      "       -1.82537645]), 'split3_test_score': array([-0.95903581, -0.95970445, -0.97635907, -2.44732053, -0.95991132,\n",
      "       -0.96082726, -0.97947702, -1.41412145, -0.97635065, -0.97921386,\n",
      "       -1.01582073, -1.36703451, -1.63473233, -1.52399593, -1.30510259,\n",
      "       -1.84394013]), 'split4_test_score': array([-0.96627941, -0.96673397, -0.9815273 , -2.08187739, -0.96706498,\n",
      "       -0.96775155, -0.98439814, -1.25894515, -0.98199574, -0.98453102,\n",
      "       -1.01819794, -1.39931048, -2.05079359, -1.25276243, -1.3934765 ,\n",
      "       -1.83265278]), 'mean_test_score': array([-0.95718936, -0.95778979, -0.97415861, -2.20098513, -0.95803735,\n",
      "       -0.95888145, -0.97718955, -1.30435479, -0.97395678, -0.97671772,\n",
      "       -1.01258662, -1.49289344, -2.12034964, -1.49502749, -1.32890138,\n",
      "       -1.83411262]), 'std_test_score': array([0.00571237, 0.00564365, 0.00493292, 0.50260993, 0.00570904,\n",
      "       0.00563521, 0.00490176, 0.06321926, 0.00544104, 0.00534732,\n",
      "       0.00458145, 0.19895201, 0.67354803, 0.28207791, 0.0435875 ,\n",
      "       0.00632518]), 'rank_test_score': array([ 1,  2,  6, 16,  3,  4,  8, 10,  5,  7,  9, 12, 15, 13, 11, 14],\n",
      "      dtype=int32), 'split0_train_score': array([-0.9056292 , -0.90647624, -0.9239581 , -1.75503395, -0.90693747,\n",
      "       -0.90801578, -0.92740846, -1.20279416, -0.92627479, -0.92921326,\n",
      "       -0.96603131, -1.33879143, -1.35594032, -1.92625471, -1.26929117,\n",
      "       -1.80127605]), 'split1_train_score': array([-0.90635996, -0.90722698, -0.92472747, -2.70602036, -0.90773077,\n",
      "       -0.90882555, -0.92820824, -1.27149306, -0.92749466, -0.93042029,\n",
      "       -0.96698498, -1.70862836, -2.02258335, -1.34002654, -1.31905996,\n",
      "       -1.80099452]), 'split2_train_score': array([-0.90620525, -0.90710049, -0.92489626, -1.52383201, -0.90755177,\n",
      "       -0.90867611, -0.92836821, -1.23167257, -0.92719096, -0.93015745,\n",
      "       -0.96714087, -1.39757928, -3.00681218, -1.2450766 , -1.23805255,\n",
      "       -1.80186429]), 'split3_train_score': array([-0.90405947, -0.90495259, -0.92277951, -2.34584105, -0.9054221 ,\n",
      "       -0.90654456, -0.92627152, -1.36243454, -0.9252401 , -0.9282073 ,\n",
      "       -0.96524688, -1.3188441 , -1.49740043, -1.4240653 , -1.25244473,\n",
      "       -1.80045943]), 'split4_train_score': array([-0.90260908, -0.90348805, -0.92127833, -1.98619036, -0.9039521 ,\n",
      "       -0.90506258, -0.92476339, -1.22092538, -0.92360487, -0.92657683,\n",
      "       -0.96369599, -1.36555282, -1.97111113, -1.21685427, -1.34245437,\n",
      "       -1.80218001]), 'mean_train_score': array([-0.90497259, -0.90584887, -0.92352793, -2.06338355, -0.90631884,\n",
      "       -0.90742492, -0.92700396, -1.25786394, -0.92596108, -0.92891502,\n",
      "       -0.96582001, -1.4258792 , -1.97076948, -1.43045549, -1.28426055,\n",
      "       -1.80135486]), 'std_train_score': array([0.00143495, 0.00143081, 0.00135034, 0.42069429, 0.00143541,\n",
      "       0.00143053, 0.001344  , 0.05692783, 0.00141665, 0.00140474,\n",
      "       0.00126384, 0.14382492, 0.57933797, 0.25846698, 0.03992249,\n",
      "       0.0006129 ]), 'mean_fit_time': array([1.86967916, 1.73139815, 1.6897264 , 1.76466079, 1.7769968 ,\n",
      "       1.9421401 , 1.77338319, 1.79728646, 1.85533509, 1.75198579,\n",
      "       1.80194988, 1.76970916, 1.80659032, 1.84772758, 1.84287443,\n",
      "       1.85005136]), 'std_fit_time': array([0.12252321, 0.03984585, 0.03479371, 0.04039381, 0.1054696 ,\n",
      "       0.09315119, 0.04463204, 0.03643218, 0.12968422, 0.04902821,\n",
      "       0.0482136 , 0.06962637, 0.03722228, 0.0442577 , 0.04171624,\n",
      "       0.08158655]), 'mean_score_time': array([0.16032782, 0.15154276, 0.1587523 , 0.17069583, 0.16325541,\n",
      "       0.16625009, 0.16398454, 0.1704865 , 0.16096487, 0.15886326,\n",
      "       0.16611495, 0.16370654, 0.17000022, 0.17829938, 0.17542315,\n",
      "       0.16780047]), 'std_score_time': array([0.00882923, 0.00365501, 0.00950024, 0.01352665, 0.00402515,\n",
      "       0.00758049, 0.00319205, 0.00752428, 0.00596467, 0.00624343,\n",
      "       0.00381554, 0.00378689, 0.00800336, 0.02284698, 0.01280264,\n",
      "       0.00848521]), 'param_lamU': masked_array(data=[0.1, 0.1, 0.1, 0.1, 1, 1, 1, 1, 10, 10, 10, 10, 100,\n",
      "                   100, 100, 100],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_lamV': masked_array(data=[0.1, 1, 10, 100, 0.1, 1, 10, 100, 0.1, 1, 10, 100, 0.1,\n",
      "                   1, 10, 100],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': ({'lamU': 0.1, 'lamV': 0.1}, {'lamU': 0.1, 'lamV': 1}, {'lamU': 0.1, 'lamV': 10}, {'lamU': 0.1, 'lamV': 100}, {'lamU': 1, 'lamV': 0.1}, {'lamU': 1, 'lamV': 1}, {'lamU': 1, 'lamV': 10}, {'lamU': 1, 'lamV': 100}, {'lamU': 10, 'lamV': 0.1}, {'lamU': 10, 'lamV': 1}, {'lamU': 10, 'lamV': 10}, {'lamU': 10, 'lamV': 100}, {'lamU': 100, 'lamV': 0.1}, {'lamU': 100, 'lamV': 1}, {'lamU': 100, 'lamV': 10}, {'lamU': 100, 'lamV': 100})}\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best K=2, RMSE=-0.9571893631715415\n",
      "{'lamU': 0.1, 'lamV': 0.1}\n",
      "{'split0_test_score': array([-0.95849969, -0.95908187, -0.97517916, -1.91647734, -0.95944245,\n",
      "       -0.96026188, -0.97825638, -1.24697082, -0.97571203, -0.97841153,\n",
      "       -1.01374915, -1.37724476, -1.38552123, -2.02520265, -1.31341168,\n",
      "       -1.83076559]), 'split1_test_score': array([-0.95061414, -0.95149337, -0.97050946, -3.01952477, -0.95152726,\n",
      "       -0.95265752, -0.97364229, -1.33661307, -0.96825875, -0.97134193,\n",
      "       -1.0099879 , -1.88824764, -2.19279159, -1.39581863, -1.36210226,\n",
      "       -1.83782816]), 'split2_test_score': array([-0.95151775, -0.9519353 , -0.96721808, -1.53972565, -0.95224076,\n",
      "       -0.95290902, -0.97017395, -1.26512344, -0.96746673, -0.97009028,\n",
      "       -1.00517737, -1.43262981, -3.33790947, -1.2773578 , -1.27041385,\n",
      "       -1.82537645]), 'split3_test_score': array([-0.95903581, -0.95970445, -0.97635907, -2.44732053, -0.95991132,\n",
      "       -0.96082726, -0.97947702, -1.41412145, -0.97635065, -0.97921386,\n",
      "       -1.01582073, -1.36703451, -1.63473233, -1.52399593, -1.30510259,\n",
      "       -1.84394013]), 'split4_test_score': array([-0.96627941, -0.96673397, -0.9815273 , -2.08187739, -0.96706498,\n",
      "       -0.96775155, -0.98439814, -1.25894515, -0.98199574, -0.98453102,\n",
      "       -1.01819794, -1.39931048, -2.05079359, -1.25276243, -1.3934765 ,\n",
      "       -1.83265278]), 'mean_test_score': array([-0.95718936, -0.95778979, -0.97415861, -2.20098513, -0.95803735,\n",
      "       -0.95888145, -0.97718955, -1.30435479, -0.97395678, -0.97671772,\n",
      "       -1.01258662, -1.49289344, -2.12034964, -1.49502749, -1.32890138,\n",
      "       -1.83411262]), 'std_test_score': array([0.00571237, 0.00564365, 0.00493292, 0.50260993, 0.00570904,\n",
      "       0.00563521, 0.00490176, 0.06321926, 0.00544104, 0.00534732,\n",
      "       0.00458145, 0.19895201, 0.67354803, 0.28207791, 0.0435875 ,\n",
      "       0.00632518]), 'rank_test_score': array([ 1,  2,  6, 16,  3,  4,  8, 10,  5,  7,  9, 12, 15, 13, 11, 14],\n",
      "      dtype=int32), 'split0_train_score': array([-0.9056292 , -0.90647624, -0.9239581 , -1.75503395, -0.90693747,\n",
      "       -0.90801578, -0.92740846, -1.20279416, -0.92627479, -0.92921326,\n",
      "       -0.96603131, -1.33879143, -1.35594032, -1.92625471, -1.26929117,\n",
      "       -1.80127605]), 'split1_train_score': array([-0.90635996, -0.90722698, -0.92472747, -2.70602036, -0.90773077,\n",
      "       -0.90882555, -0.92820824, -1.27149306, -0.92749466, -0.93042029,\n",
      "       -0.96698498, -1.70862836, -2.02258335, -1.34002654, -1.31905996,\n",
      "       -1.80099452]), 'split2_train_score': array([-0.90620525, -0.90710049, -0.92489626, -1.52383201, -0.90755177,\n",
      "       -0.90867611, -0.92836821, -1.23167257, -0.92719096, -0.93015745,\n",
      "       -0.96714087, -1.39757928, -3.00681218, -1.2450766 , -1.23805255,\n",
      "       -1.80186429]), 'split3_train_score': array([-0.90405947, -0.90495259, -0.92277951, -2.34584105, -0.9054221 ,\n",
      "       -0.90654456, -0.92627152, -1.36243454, -0.9252401 , -0.9282073 ,\n",
      "       -0.96524688, -1.3188441 , -1.49740043, -1.4240653 , -1.25244473,\n",
      "       -1.80045943]), 'split4_train_score': array([-0.90260908, -0.90348805, -0.92127833, -1.98619036, -0.9039521 ,\n",
      "       -0.90506258, -0.92476339, -1.22092538, -0.92360487, -0.92657683,\n",
      "       -0.96369599, -1.36555282, -1.97111113, -1.21685427, -1.34245437,\n",
      "       -1.80218001]), 'mean_train_score': array([-0.90497259, -0.90584887, -0.92352793, -2.06338355, -0.90631884,\n",
      "       -0.90742492, -0.92700396, -1.25786394, -0.92596108, -0.92891502,\n",
      "       -0.96582001, -1.4258792 , -1.97076948, -1.43045549, -1.28426055,\n",
      "       -1.80135486]), 'std_train_score': array([0.00143495, 0.00143081, 0.00135034, 0.42069429, 0.00143541,\n",
      "       0.00143053, 0.001344  , 0.05692783, 0.00141665, 0.00140474,\n",
      "       0.00126384, 0.14382492, 0.57933797, 0.25846698, 0.03992249,\n",
      "       0.0006129 ]), 'mean_fit_time': array([1.86967916, 1.73139815, 1.6897264 , 1.76466079, 1.7769968 ,\n",
      "       1.9421401 , 1.77338319, 1.79728646, 1.85533509, 1.75198579,\n",
      "       1.80194988, 1.76970916, 1.80659032, 1.84772758, 1.84287443,\n",
      "       1.85005136]), 'std_fit_time': array([0.12252321, 0.03984585, 0.03479371, 0.04039381, 0.1054696 ,\n",
      "       0.09315119, 0.04463204, 0.03643218, 0.12968422, 0.04902821,\n",
      "       0.0482136 , 0.06962637, 0.03722228, 0.0442577 , 0.04171624,\n",
      "       0.08158655]), 'mean_score_time': array([0.16032782, 0.15154276, 0.1587523 , 0.17069583, 0.16325541,\n",
      "       0.16625009, 0.16398454, 0.1704865 , 0.16096487, 0.15886326,\n",
      "       0.16611495, 0.16370654, 0.17000022, 0.17829938, 0.17542315,\n",
      "       0.16780047]), 'std_score_time': array([0.00882923, 0.00365501, 0.00950024, 0.01352665, 0.00402515,\n",
      "       0.00758049, 0.00319205, 0.00752428, 0.00596467, 0.00624343,\n",
      "       0.00381554, 0.00378689, 0.00800336, 0.02284698, 0.01280264,\n",
      "       0.00848521]), 'param_lamU': masked_array(data=[0.1, 0.1, 0.1, 0.1, 1, 1, 1, 1, 10, 10, 10, 10, 100,\n",
      "                   100, 100, 100],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_lamV': masked_array(data=[0.1, 1, 10, 100, 0.1, 1, 10, 100, 0.1, 1, 10, 100, 0.1,\n",
      "                   1, 10, 100],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': ({'lamU': 0.1, 'lamV': 0.1}, {'lamU': 0.1, 'lamV': 1}, {'lamU': 0.1, 'lamV': 10}, {'lamU': 0.1, 'lamV': 100}, {'lamU': 1, 'lamV': 0.1}, {'lamU': 1, 'lamV': 1}, {'lamU': 1, 'lamV': 10}, {'lamU': 1, 'lamV': 100}, {'lamU': 10, 'lamV': 0.1}, {'lamU': 10, 'lamV': 1}, {'lamU': 10, 'lamV': 10}, {'lamU': 10, 'lamV': 100}, {'lamU': 100, 'lamV': 0.1}, {'lamU': 100, 'lamV': 1}, {'lamU': 100, 'lamV': 10}, {'lamU': 100, 'lamV': 100})}\n"
     ]
    }
   ],
   "source": [
    "#Tune hyper-params with Grid Search\n",
    "\n",
    "MAX_ITER=100\n",
    "#Tune regularization hyper-params\n",
    "regu_params = {\"lamU\":[0.1,1,10,100],\"lamV\":[0.1,1,10,100]}\n",
    "pmfEst = PMF(K=2, maxIter=MAX_ITER)\n",
    "gs=GridSearchCV(pmfEst, regu_params, cv=5, refit=True, verbose=1)\n",
    "gs.fit(trainData)\n",
    "\n",
    "bestLamU = gs.best_params_['lamU']\n",
    "bestLamV = gs.best_params_['lamV']\n",
    "#Mean cross-validated score of the best_estimator\n",
    "bestScoreL = -gs.best_score_\n",
    "print(\"best lamU {}, best lamV {}, RMSE={}\".format(bestLamU, bestLamV,bestScoreL ))\n",
    "print(gs.best_params_)\n",
    "print(gs.cv_results_)\n",
    "\n",
    "#Tune # latent features\n",
    "factors_params = {\"K\":[1,2,3,4,5]}\n",
    "pmfEst2 = PMF(lamU=bestLamU,lamV=bestLamV, maxIter=MAX_ITER)\n",
    "gs2 =GridSearchCV(pmfEst, factors_params, cv=5, refit=True, verbose=1)\n",
    "gs2.fit(trainData)\n",
    "\n",
    "bestK = gs2.best_params_['K']\n",
    "bestScoreK = -gs2.best_score_\n",
    "print(\"best K={}, RMSE={}\".format(bestK,bestScoreK ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  80 out of  80 | elapsed:  2.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish tunning lamda U and lamda V \r\n",
      "==> Best lamU 0.1, best lamV 0.1, RMSE=1.0722257920860467\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:   39.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish tunning factors K \n",
      "==> Best K=5, RMSE=1.0379674940307944\n",
      "Epoch 0: Train RMSE: 2.3937\n",
      "Epoch 1: Train RMSE: 2.1956\n",
      "Epoch 2: Train RMSE: 2.0086\n",
      "Epoch 3: Train RMSE: 1.8406\n",
      "Epoch 4: Train RMSE: 1.6968\n",
      "Epoch 5: Train RMSE: 1.5783\n",
      "Epoch 6: Train RMSE: 1.4831\n",
      "Epoch 7: Train RMSE: 1.4074\n",
      "Epoch 8: Train RMSE: 1.3469\n",
      "Epoch 9: Train RMSE: 1.2980\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.3697991719776725"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# debug session\n",
    "Experiment(testData,trainData,verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train RMSE: 2.7059\t Test RMSE: 2.7170\n",
      "Epoch 1: Train RMSE: 2.1115\t Test RMSE: 2.1261\n",
      "Epoch 2: Train RMSE: 1.7169\t Test RMSE: 1.7300\n",
      "Epoch 3: Train RMSE: 1.4985\t Test RMSE: 1.5105\n",
      "Epoch 4: Train RMSE: 1.3645\t Test RMSE: 1.3766\n",
      "Epoch 5: Train RMSE: 1.2731\t Test RMSE: 1.2862\n",
      "Epoch 6: Train RMSE: 1.2072\t Test RMSE: 1.2214\n",
      "Epoch 7: Train RMSE: 1.1577\t Test RMSE: 1.1732\n",
      "Epoch 8: Train RMSE: 1.1197\t Test RMSE: 1.1364\n",
      "Epoch 9: Train RMSE: 1.0897\t Test RMSE: 1.1077\n"
     ]
    }
   ],
   "source": [
    "#linear running time in observed ratings. using the sparsity\n",
    "np.random.seed(seed=1)\n",
    "U= np.random.random_sample((K, USER_ID_MAX))\n",
    "V= np.random.random_sample((K, ITEM_ID_MAX))\n",
    "MAX_ITER2=2\n",
    "for epoch in range(MAX_ITER2):\n",
    "    # linear algo. Batch Gradient Descent\n",
    "    Ut = U\n",
    "    Vt = V\n",
    "    for (userID,itemID,rij) in trainData:\n",
    "        i = userID - 1\n",
    "        j = itemID - 1\n",
    "        rij_pred = np.dot((U.T)[i,:],V[:,j])\n",
    "        diff_ij = rij - rij_pred\n",
    "        #partially update the latent features of user i and item j\n",
    "        Ut[:,i] += - lr * (-V[:,j] * diff_ij) \n",
    "        Vt[:,j] += - lr * (-U[:,i] * diff_ij)\n",
    "        \n",
    "        #rmseTrain[epoch] += diff_ij**2\n",
    "    #update U and V\n",
    "    Ut = Ut - lr * ( lamU * U)\n",
    "    Vt = Vt - lr * (lamV * V)\n",
    "    U = Ut\n",
    "    V = Vt\n",
    "    \n",
    "    #evaluate\n",
    "    #rmseTrain[epoch] = np.sqrt(rmseTrain[epoch]/len(trainData))\n",
    "    R_pred = U.T @ V\n",
    "    diffTrain = indicator * (R_train - R_pred)\n",
    "    rmseTrain[epoch] = np.sqrt( (diffTrain**2).sum()/indicator.sum())\n",
    "    diffTest = indicatoR_test * (R_test - R_pred)\n",
    "    rmseTest[epoch] = np.sqrt( (diffTest**2).sum()/indicatoR_test.sum())\n",
    "    print(\"Epoch {}: Train RMSE: {:.4f}\\t Test RMSE: {:.4f}\".format(epoch, rmseTrain[epoch], rmseTest[epoch]))\n",
    "    \n",
    "###===> The running time is longer than that of using the complete matrix    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 1682)\n",
      "(2, 1682)\n",
      "(2, 943)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "80000"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for python test\n",
    "np.random.seed(seed=1)\n",
    "a = np.random.random_sample((2,2))\n",
    "b = (a > 0).astype(int)\n",
    "b[0,0]=0\n",
    "print(RMatrix.shape)\n",
    "print(V.shape)\n",
    "print(U.shape)\n",
    "dt=np.asarray([1,1])\n",
    "indicator.sum()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
